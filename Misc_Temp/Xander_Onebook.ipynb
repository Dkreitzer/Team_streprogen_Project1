{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "#Dependencies#\n",
    "##############\n",
    "\n",
    "import os                                        ### import operating system ###\n",
    "import xml.etree.ElementTree as ET               ### xml.etree is a flexible container object,\n",
    "import gzip                                      ### compress and decompress gzip files ###\n",
    "import time                                      ### import time libraries ###\n",
    "import requests                                  ### Libraries to support HTML requests in python ###\n",
    "\n",
    "\n",
    "##############\n",
    "#Dependencies VISUALS#\n",
    "##############\n",
    "%matplotlib notebook\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Defenition to pull Incident Reports and Traffic Detectors from MN DOT\n",
    "####################################################################\n",
    "# Request incident information - xml.gz file\n",
    "# Open, decompress, and decode\n",
    "# Request traffic detector information - xml.gz file\n",
    "# Open, decompress, and decode\n",
    "\n",
    "def download():\n",
    "    i = requests.get('http://data.dot.state.mn.us/iris_xml/incident.xml.gz')\n",
    "    with open('data/XMLs/incidents.xml', 'w') as handle:\n",
    "        handle.write(gzip.decompress(i.content).decode('utf-8'))\n",
    "\n",
    "    s = requests.get('http://data.dot.state.mn.us/iris_xml/stat_sample.xml.gz')\n",
    "    with open('data/XMLs/station_sample.xml', 'w') as handle:\n",
    "        handle.write(gzip.decompress(s.content).decode('ISO-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Defenition to convert information in DataFrames\n",
    "###################################################\n",
    "# Identify crash information, try to open csv file and convert to DF, save updated DF as csv\n",
    "# Identify detector information, try to open as csv and convert to DF, save updated DF as csv\n",
    "\n",
    "\n",
    "\n",
    "def data_check():\n",
    "\n",
    "        try:\n",
    "            with open('data/crash_data.csv', 'r') as CD:\n",
    "                incidents()\n",
    "        except FileNotFoundError:\n",
    "                All_Crash_Data = pandas.DataFrame(columns=['Name', 'Date', 'DirectionLocation', 'Road', 'Event'])\n",
    "                with open('data/crash_data.csv', 'w') as f:\n",
    "                    All_Crash_Data.to_csv(f, header=True)\n",
    "                    incidents()\n",
    "\n",
    "        try:\n",
    "            with open('data/station_data.csv', 'r') as CD:\n",
    "                stations()\n",
    "        except FileNotFoundError:\n",
    "                station_data = pandas.DataFrame(columns=  [\"Station\",\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\",\"Lat\",\"Lng\"])\n",
    "                with open('data/station_data.csv', 'w') as f:\n",
    "                    station_data.to_csv(f, header=True)\n",
    "                    stations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Parse incident information and save into csv\n",
    "###################################################\n",
    "\n",
    "## Create lists, append lists if data exists otherwise enter NA, combine data as DF, save as csv\n",
    "\n",
    "def incidents():\n",
    "    dates = []\n",
    "    incident_dirs = []\n",
    "    roads = []\n",
    "    locations = []\n",
    "    names = []\n",
    "    events = []\n",
    "\n",
    "    XMLfile = \"data/XMLs/incidents.xml\"\n",
    "    parsedXML = ET.parse(XMLfile)\n",
    "    root = parsedXML.getroot()\n",
    "    for child in root:\n",
    "        try:\n",
    "            dates.append(child.attrib['event_date'])\n",
    "        except KeyError:\n",
    "            dates.append(\"NA\")\n",
    "        try:\n",
    "            names.append(str(child.attrib['name']))\n",
    "        except KeyError:\n",
    "            name.append(\"NA\")\n",
    "        try:\n",
    "            incident_dirs.append(child.attrib['dir'])\n",
    "        except KeyError:\n",
    "            incident_dir.append(\"NA\")\n",
    "        try:\n",
    "            roads.append(child.attrib['road'])\n",
    "        except KeyError:\n",
    "            roads.append('NA')\n",
    "        try:\n",
    "            locations.append(child.attrib['location'])\n",
    "        except KeyError:\n",
    "            locations.append(\"NA\")\n",
    "        try: \n",
    "            event = child.attrib['event_type'].split(\"_\", 1)\n",
    "            events.append(event[1])\n",
    "        except KeyError:\n",
    "            events.append(\"NA\")\n",
    "\n",
    "\n",
    "    DF = pandas.DataFrame({\"Name\" : names,\n",
    "                       \"Date\" : dates,\n",
    "                       \"Direction\": incident_dirs,\n",
    "                       \"Road\" : roads,\n",
    "                       \"Location\" : locations,\n",
    "                       \"Event\" : events})\n",
    "\n",
    "\n",
    "    print(\"Incident Data Parsed\")\n",
    "\n",
    "    with open('data/crash_data.csv', 'a') as f:\n",
    "        DF.to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Parse station information and save as csv\n",
    "###################################################\n",
    "\n",
    "## Create lists, append lists if data exists otherwise enter NA, combine data as DF, save as csv\n",
    "def stations():\n",
    "    stations = []\n",
    "    times = []\n",
    "    flows = []\n",
    "    speeds = []\n",
    "    order = []\n",
    "    headings = []\n",
    "    lats = []\n",
    "    lngs = []\n",
    "    with open('station_keys/Northbound_35W_StationNames.csv', 'r') as NB:\n",
    "        \n",
    "        NB_DF = pandas.read_csv(NB)\n",
    "    with open('station_keys/Southbound_35W_StationNames.csv', 'r') as SB:\n",
    "        SB_DF = pandas.read_csv(SB)\n",
    "    \n",
    "    XMLfile = \"data/XMLs/station_sample.xml\"\n",
    "    parsedXML = ET.parse(XMLfile)\n",
    "    root = parsedXML.getroot()\n",
    "    for child in root:\n",
    "    \n",
    "        if child.attrib['sensor'] in NB_DF[\"1\"].values :\n",
    "            lats.append(NB_DF.loc[NB_DF['1'] == child.attrib['sensor']]['Lat'].values[0])\n",
    "            lngs.append(NB_DF.loc[NB_DF['1'] == child.attrib['sensor']]['Lng'].values[0])\n",
    "        \n",
    "            headings.append(\"NB\")\n",
    "            order.append(NB_DF.loc[NB_DF['1'] == child.attrib['sensor']]['Order'].values[0])\n",
    "            try:\n",
    "                stations.append(child.attrib['sensor'])\n",
    "            except KeyError:\n",
    "                stations.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                times.append(str(root.attrib['time_stamp']))\n",
    "            except KeyError:\n",
    "                times.append(\"NA\")\n",
    "            try:\n",
    "                flows.append(child.attrib['flow'])\n",
    "            except KeyError:\n",
    "                flows.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                speeds.append(child.attrib['speed'])\n",
    "            except KeyError:\n",
    "                speeds.append(\"NA\")\n",
    "           \n",
    "        if child.attrib['sensor'] in SB_DF[\"1\"].values:\n",
    "            lats.append(SB_DF.loc[SB_DF['1'] == child.attrib['sensor']]['Lat'].values[0])\n",
    "            lngs.append(SB_DF.loc[SB_DF['1'] == child.attrib['sensor']]['Lng'].values[0])\n",
    "            headings.append(\"SB\")\n",
    "            order.append(SB_DF.loc[SB_DF['1'] == child.attrib['sensor']]['Order'].values[0])\n",
    "            try:\n",
    "                stations.append(child.attrib['sensor'])\n",
    "            except KeyError:\n",
    "                stations.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                times.append(str(root.attrib['time_stamp']))\n",
    "            except KeyError:\n",
    "                times.append(\"NA\")\n",
    "            try:\n",
    "                flows.append(child.attrib['flow'])\n",
    "            except KeyError:\n",
    "                flows.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                speeds.append(child.attrib['speed'])\n",
    "            except KeyError:\n",
    "                speeds.append(\"NA\")\n",
    "            \n",
    "\n",
    "    DF = pandas.DataFrame({\"Station\" : stations,\n",
    "                       \"Heading\": headings,\n",
    "                        \"Time\" : times,\n",
    "                       \"Order\" : order,\n",
    "                       \"Speed\" : speeds,\n",
    "                       \"Flow\" : flows,\n",
    "                      \"Lat\": lats,\n",
    "                      \"Lng\" : lngs })\n",
    "    with open(f'data/station_data.csv', 'w') as f:\n",
    "           DF.to_csv(f, header=True)\n",
    "    print(\"Station Data Parsed\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Route_Summary():\n",
    "    try:\n",
    "        Summary = pandas.read_csv('data/Route_Summary.csv')\n",
    "    except FileNotFoundError:\n",
    "        Summary = pandas.DataFrame(columns=[\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\",\"Lat\",\"Lng\"])\n",
    "\n",
    "\n",
    "        \n",
    "    All_Station_Data = pandas.read_csv('data/station_data.csv')\n",
    "#     All_Station_Data = All_Station_Data.set_index('Station')\n",
    "    \n",
    "\n",
    "    route = All_Station_Data.groupby('Station').head(1).index.values\n",
    "\n",
    "    for station in route:\n",
    "            Summary_partial = All_Station_Data.loc[station, \n",
    "                                                       [\"Station\",\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\",\"Lat\",\"Lng\"]]\n",
    "            Summary = Summary.append(Summary_partial,sort=True)\n",
    "            Summary = Summary.replace(\"UNKNOWN\",0)\n",
    "            \n",
    " \n",
    "    Summary = Summary.sort_values(['Station', 'Time'])\n",
    "    with open('data/Route_Summary.csv', 'w') as f:\n",
    "        Summary.to_csv(f,header=True, columns=[\"Station\",\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\",\"Lat\",\"Lng\"])\n",
    "                       \n",
    "    print(\"Summary Saved at data/Route_Summary.csv\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    lats = []\n",
    "    lngs = []\n",
    "    station_list = []\n",
    "        \n",
    "    XMLfile = \"data/XMLs/station_config.xml\"\n",
    "    parsedXML = ET.parse(XMLfile)\n",
    "    root = parsedXML.getroot()\n",
    "      \n",
    "\n",
    "    for i in root.iter('corridor'):\n",
    "        for child in i:\n",
    "            try:\n",
    "                station_list.append(child.attrib['station_id'])\n",
    "\n",
    "            except KeyError:\n",
    "                station_list.append(\"no ID\")\n",
    "            try:\n",
    "                lats.append(child.attrib['lat'])\n",
    "            except KeyError:\n",
    "                 lats.append(\"no ID\")\n",
    "            try:\n",
    "                lngs.append(child.attrib['lon'])\n",
    "            except KeyError:\n",
    "                lngs.append(\"no ID\")\n",
    "\n",
    "\n",
    "\n",
    "    DF = pandas.DataFrame({ \"Station\":station_list,\n",
    "    #                        \"Label\":decription,\n",
    "                       \"Lat\":lats, \"Lng\":lngs,})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open('data/station_config.csv', 'w') as f:\n",
    "        DF.to_csv(f, header=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Identify metro sensor configurations\n",
    "############################################\n",
    "# Request xml.gz file, decompress, decode\n",
    "# with the stat_config.xml, look for a matching station. If not found, write the new station ID to stat_config.csv\n",
    "try:\n",
    "        config()\n",
    "except FileNotFoundError:\n",
    "    c = requests.get('http://data.dot.state.mn.us/iris_xml/metro_config.xml.gz')\n",
    "    with open('data/XMLs/station_config.xml', 'w') as handle:\n",
    "        handle.write(gzip.decompress(c.content).decode('utf-8'))\n",
    "    Station_Config = pandas.DataFrame(columns=['Station', 'Lat', 'Lng'])\n",
    "    with open('data/station_config.csv', 'w') as f:\n",
    "        Station_Config.to_csv(f, header=True)\n",
    "        config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "#If the program is still running, \n",
    "# Print the download is complete\n",
    "# Print the Parsing is Complete\n",
    "# Program sleep for 30 seconds\n",
    "# ####################################\n",
    "\n",
    "def Data_Request():\n",
    "    while True:\n",
    "        download()\n",
    "        data_check()\n",
    "        Route_Summary()\n",
    "        print(\"sleeping 30s\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_summary():\n",
    "    global route_df\n",
    "    global Times\n",
    "    route_df= pandas.read_csv('Data/route_summary.csv')\n",
    "    route_df = route_df.drop_duplicates()\n",
    "    route_df = route_df.set_index(\"Station\")\n",
    "    route_df= route_df.fillna(0)\n",
    "    route_df = route_df.drop(\"Unnamed: 0\", axis=1)\n",
    "    Times = np.unique(route_df[\"Time\"])\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f'Results/maps/{datetime.now().strftime(\"%b%d\")}')\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Daily_Visuals():\n",
    "    start_time = datetime.now().strftime(\"%b%d_%H_%M_%S\")\n",
    "    route_timed = route_df.reset_index().set_index([\"Time\"])\n",
    "    print(f\"Starting Visualization at {start_time}\")\n",
    "    for time in Times:\n",
    "        Timed_Map(time)\n",
    "    end_time = datetime.now().strftime(\"%b%d_%H_%M_%S\")\n",
    "    print(f\"Visualization completed at {end_time}\")\n",
    "    print(f\"It took {end_time} - {start_time} to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Timed_Map(Datetimestring):\n",
    "    global in_time\n",
    "    in_time = Datetimestring\n",
    "    in_time = ''.join(in_time.split()[1:4]).replace(\":\", \"_\")\n",
    "    route_timed_in = route_df.reset_index().set_index([\"Time\"])\n",
    "    route_timed = route_timed_in.loc[[Datetimestring]]\n",
    "    route_timed_out = route_timed.reset_index().set_index([\"Station\"])\n",
    "    grab_timed_data(route_timed_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_timed_data(DataFrame):\n",
    "    global Results_List\n",
    "    global ResultsNB\n",
    "    global ResultsSB\n",
    "    route = DataFrame.groupby('Station').head(1).index.values\n",
    "    Results = {}\n",
    "\n",
    "    for station in route:\n",
    "        try:\n",
    "            Flow =  float(DataFrame.loc[station,'Flow'])\n",
    "            Speed = int(DataFrame.loc[station,'Speed'])\n",
    "            Lng =DataFrame.loc[station,'Lng']\n",
    "            Lat = DataFrame.loc[station,'Lat']\n",
    "            Order = DataFrame.loc[station,'Order'].astype(dtype=\"int\")\n",
    "            Heading = DataFrame.loc[station,'Heading']\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            Results.update({station : {'Station' :station,\n",
    "                \"Heading\" : Heading,\n",
    "                \"Order\" : Order,\n",
    "                \"Current Speed\" : Speed,\n",
    "                \"Current Flow\" : Flow,\n",
    "        \n",
    "                \"Lat\":Lat, \n",
    "                \"Lng\":Lng}})\n",
    "\n",
    "\n",
    "        except ValueError as v:\n",
    "            print(f\"{station} {v}\")\n",
    "    Results = pandas.DataFrame(Results).T\n",
    "    Results = Results.sort_values(['Heading', 'Order'])\n",
    "    Results = Results.set_index(['Heading', 'Order'], drop=True)\n",
    "    Results.head()\n",
    "    ResultsNB = Results.xs('NB', level='Heading')\n",
    "    ResultsSB = Results.xs('NB', level='Heading')\n",
    "    Results_List= {\"NB\":ResultsNB,\"SB\":ResultsSB}\n",
    "    mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping():\n",
    "            \n",
    "    global folium_map\n",
    "    for result in Results_List:\n",
    "\n",
    "        x = int(len(Results_List[result]['Station']) / 2)\n",
    "\n",
    "        folium_map = folium.Map((Results_List[result].iloc[x, 2],ResultsNB.iloc[x,3]),\n",
    "\n",
    "                                zoom_start=11,\n",
    "                                 tiles=\"CartoDB positron\")\n",
    "        Features = []\n",
    "        Last_Sensor = []\n",
    "        for index, row in Results_List[result].iterrows():\n",
    "            if row['Current Speed'] < 15:\n",
    "                color = \"#ff0000\"\n",
    "            elif row['Current Speed'] >= 15 and row['Current Speed'] < 30:\n",
    "                color =  \"#ffa500\"\n",
    "            elif row['Current Speed'] >= 30 and row['Current Speed'] < 55:\n",
    "                color = \"#ffff00\"\n",
    "            else:\n",
    "                color = \"#008000\"\n",
    "\n",
    "            weight = row['Current Flow'] / 200\n",
    "            if row['Current Flow'] == 0:\n",
    "                weight = 1\n",
    "                color = \"#808080\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            Current_Sensor = (row['Lat'], row['Lng'])\n",
    "\n",
    "            if Last_Sensor == [] :\n",
    "                Last_Sensor = (row['Lat'], row['Lng'])\n",
    "            else:\n",
    "                if row['Current Flow'] != 0:\n",
    "                    weight = row['Current Flow'] / 200\n",
    "\n",
    "                folium.PolyLine([Current_Sensor,Last_Sensor],\n",
    "                 weight=weight,color=color,\n",
    "                                popup=f\"Time:{timenow} Speed:{row['Current Speed']} Flow: {row['Current Flow']}\").add_to(folium_map)\n",
    "\n",
    "                Last_Sensor = (row['Lat'], row['Lng'])\n",
    "\n",
    "            folium.CircleMarker(location=(Current_Sensor),\n",
    "                                radius=3,\n",
    "                                popup=(\"station =\"  + row['Station']), fill=False).add_to(folium_map)\n",
    "\n",
    "        folium_map.save(f\"results/maps/routemap_temp.html\")\n",
    "        print(f'Map saved at results/maps/routemap_temp.html')\n",
    "        delay=7\n",
    "        fn=f'results/maps/routemap_temp.html'\n",
    "        tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "\n",
    "        browser = webdriver.Firefox()\n",
    "        browser.get(tmpurl)\n",
    "        #Give the map tiles some time to load\n",
    "        time.sleep(delay)\n",
    "        try: \n",
    "            browser.save_screenshot(f'results/maps/{datetime.now().strftime(\"%b%d\")}/{result}/{result}routemap{in_time}.png')\n",
    "            print(f'Map Converted -->> results/maps/{datetime.now().strftime(\"%b%d\")}/{result}/{result}routemap{in_time}')\n",
    "        except NameError:\n",
    "            \n",
    "            browser.save_screenshot(f'results/maps/{datetime.now().strftime(\"%b%d\")}/{result}/{result}routemap{timenow}.png')\n",
    "            print(f'Map Converted -->> results/maps/{datetime.now().strftime(\"%b%d\")}/{result}/{result}routemap{timenow}')\n",
    "        browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "timenow = datetime.now().strftime(\"%b%d_%H_%M_%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_recent_map():\n",
    "    download()\n",
    "    data_check()\n",
    "    Route_Summary()\n",
    "    import_summary()\n",
    "    recent_data = route_df.groupby('Station').last()\n",
    "    grab_timed_data(recent_data)\n",
    "    folium_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map_Request_Timed(Timestring):\n",
    "    import_summary()\n",
    "    Timed_Map(Timestring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Daily_PNGs():\n",
    "    import_summary()\n",
    "    Daily_Visuals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gif_request(date):\n",
    "    ##format is oct01##\n",
    "    NBpngs = str(os.listdir(f\"Results/Maps/{date}/NB\"))\n",
    "    SBpngs = str(os.listdir(f\"Results/Maps/{date}/SB\"))\n",
    "    NBpngs = NBpngs.replace(\"'\",\"\")\n",
    "    NBpngs = NBpngs.replace(\",\",\"\")\n",
    "    SBpngs = SBpngs.replace(\"'\",\"\")\n",
    "    SBpngs = SBpngs.replace(\",\",\"\")\n",
    "    print(\"COPY THIS INTO TERMINAL AT NBpngs Folders NO BRACKETS\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"convert -loop 0 -delay 60 {NBpngs} NBMap.gif\\n\\n\")\n",
    "    print(\"COPY THIS INTO TERMINAL AT SBpngs Folders NO BRACKETS\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"convert -loop 0 -delay 60 {SBpngs} SBMap.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Visualization at Oct11_16_59_04\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/NB/NBroutemapOct1116_45_29\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/SB/SBroutemapOct1116_45_29\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/NB/NBroutemapOct1116_48_59\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/SB/SBroutemapOct1116_48_59\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/NB/NBroutemapOct1116_49_59\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/SB/SBroutemapOct1116_49_59\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/NB/NBroutemapOct1116_50_59\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/SB/SBroutemapOct1116_50_59\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/NB/NBroutemapOct1116_53_29\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/SB/SBroutemapOct1116_53_29\n",
      "Map saved at results/maps/routemap_temp.html\n",
      "Map Converted -->> results/maps/Oct11/NB/NBroutemapOct1116_54_29\n",
      "Map saved at results/maps/routemap_temp.html\n"
     ]
    }
   ],
   "source": [
    "# most_recent_map()\n",
    "# gif_request('oct11')\n",
    "Daily_PNGs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY THIS INTO TERMINAL AT NBpngs Folders NO BRACKETS\n",
      "------------------------------------------------\n",
      "convert -loop 0 -delay 60 [NBroutemapOct11_16_47_54.png] NBMap.gif\n",
      "\n",
      "\n",
      "COPY THIS INTO TERMINAL AT SBpngs Folders NO BRACKETS\n",
      "------------------------------------------------\n",
      "convert -loop 0 -delay 60 [SBroutemapOct11_16_47_54.png] SBMap.gif\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
