{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "#Dependencies#\n",
    "##############\n",
    "\n",
    "import pandas as pd                              ### import pandas ###\n",
    "import os                                        ### import operating system ###\n",
    "import xml.etree.ElementTree as ET               ### xml.etree is a flexible container object,\n",
    "import gzip                                      ### compress and decompress gzip files ###\n",
    "import time                                      ### import time libraries ###\n",
    "import shutil                                    ### Higher level copying and archiving ###\n",
    "import requests                                  ### Libraries to support HTML requests in python ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Defenition to pull Incident Reports and Traffic Detectors from MN DOT\n",
    "####################################################################\n",
    "# Request incident information - xml.gz file\n",
    "# Open, decompress, and decode\n",
    "# Request traffic detector information - xml.gz file\n",
    "# Open, decompress, and decode\n",
    "\n",
    "def download():\n",
    "    i = requests.get('http://data.dot.state.mn.us/iris_xml/incident.xml.gz')\n",
    "    with open('data/XMLs/incidents.xml', 'w') as handle:\n",
    "        handle.write(gzip.decompress(i.content).decode('utf-8'))\n",
    "\n",
    "    s = requests.get('http://data.dot.state.mn.us/iris_xml/stat_sample.xml.gz')\n",
    "    with open('data/XMLs/station_sample.xml', 'w') as handle:\n",
    "        handle.write(gzip.decompress(s.content).decode('ISO-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Defenition to convert information in DataFrames\n",
    "###################################################\n",
    "# Identify crash information, try to open csv file and convert to DF, save updated DF as csv\n",
    "# Identify detector information, try to open as csv and convert to DF, save updated DF as csv\n",
    "\n",
    "\n",
    "\n",
    "def data_check():\n",
    "\n",
    "        try:\n",
    "            with open('data/crash_data.csv', 'r') as CD:\n",
    "                incidents()\n",
    "        except FileNotFoundError:\n",
    "                All_Crash_Data = pd.DataFrame(columns=['Name', 'Date', 'DirectionLocation', 'Road', '', 'Event'])\n",
    "                with open('data/crash_data.csv', 'w') as f:\n",
    "                    All_Crash_Data.to_csv(f, header=True)\n",
    "                    incidents()\n",
    "\n",
    "        try:\n",
    "            with open('data/station_data.csv', 'r') as CD:\n",
    "                stations()\n",
    "        except FileNotFoundError:\n",
    "                station_data = pd.DataFrame(columns=['Station', 'Time', 'Occupancy', 'Speed', 'Flow'])\n",
    "                with open('data/station_data.csv', 'w') as f:\n",
    "                    station_data.to_csv(f, header=True)\n",
    "                    stations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Parse incident information and save into csv\n",
    "###################################################\n",
    "\n",
    "## Create lists, append lists if data exists otherwise enter NA, combine data as DF, save as csv\n",
    "\n",
    "def incidents():\n",
    "    dates = []\n",
    "    incident_dirs = []\n",
    "    roads = []\n",
    "    locations = []\n",
    "    names = []\n",
    "    events = []\n",
    "\n",
    "    XMLfile = \"data/XMLs/incidents.xml\"\n",
    "    parsedXML = ET.parse(XMLfile)\n",
    "    root = parsedXML.getroot()\n",
    "    for child in root:\n",
    "        try:\n",
    "            dates.append(child.attrib['event_date'])\n",
    "        except KeyError:\n",
    "            dates.append(\"NA\")\n",
    "        try:\n",
    "            names.append(str(child.attrib['name']))\n",
    "        except KeyError:\n",
    "            name.append(\"NA\")\n",
    "        try:\n",
    "            incident_dirs.append(child.attrib['dir'])\n",
    "        except KeyError:\n",
    "            incident_dir.append(\"NA\")\n",
    "        try:\n",
    "            roads.append(child.attrib['road'])\n",
    "        except KeyError:\n",
    "            roads.append('NA')\n",
    "        try:\n",
    "            locations.append(child.attrib['location'])\n",
    "        except KeyError:\n",
    "            locations.append(\"NA\")\n",
    "        try: \n",
    "            event = child.attrib['event_type'].split(\"_\", 1)\n",
    "            events.append(event[1])\n",
    "        except KeyError:\n",
    "            events.append(\"NA\")\n",
    "\n",
    "\n",
    "    DF = pd.DataFrame({\"Name\" : names,\n",
    "                       \"Date\" : dates,\n",
    "                       \"Direction\": incident_dirs,\n",
    "                       \"Road\" : roads,\n",
    "                       \"Location\" : locations,\n",
    "                       \"Event\" : events})\n",
    "\n",
    "\n",
    "    print(\"Incident Data Parsed\")\n",
    "\n",
    "    with open('data/crash_data.csv', 'a') as f:\n",
    "        DF.to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Parse station information and save as csv\n",
    "###################################################\n",
    "\n",
    "## Create lists, append lists if data exists otherwise enter NA, combine data as DF, save as csv\n",
    "def stations():\n",
    "    global NB_DF\n",
    "    global SB_DF\n",
    "    global DF\n",
    "    stations = []\n",
    "    times = []\n",
    "    flows = []\n",
    "    speeds = []\n",
    "    order = []\n",
    "    headings = []\n",
    "    with open('station_keys/Northbound_35W_StationNames.csv', 'r') as NB:\n",
    "        \n",
    "        NB_DF = pd.read_csv(NB)\n",
    "    with open('station_keys/Southbound_35W_StationNames.csv', 'r') as SB:\n",
    "        SB_DF = pd.read_csv(SB)\n",
    "    \n",
    "    XMLfile = \"data/XMLs/station_sample.xml\"\n",
    "    parsedXML = ET.parse(XMLfile)\n",
    "    root = parsedXML.getroot()\n",
    "    for child in root:\n",
    "    \n",
    "        \n",
    "     \n",
    "        if child.attrib['sensor'] in NB_DF[\"1\"].values :\n",
    "            headings.append(\"NB\")\n",
    "            order.append(NB_DF.loc[NB_DF['1'] == child.attrib['sensor']]['Order'].values[0])\n",
    "            try:\n",
    "                stations.append(child.attrib['sensor'])\n",
    "            except KeyError:\n",
    "                stations.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                times.append(str(root.attrib['time_stamp']))\n",
    "            except KeyError:\n",
    "                times.append(\"NA\")\n",
    "            try:\n",
    "                flows.append(child.attrib['flow'])\n",
    "            except KeyError:\n",
    "                flows.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                speeds.append(child.attrib['speed'])\n",
    "            except KeyError:\n",
    "                speeds.append(\"NA\")\n",
    "           \n",
    "        if child.attrib['sensor'] in SB_DF[\"1\"].values :\n",
    "            headings.append(\"SB\")\n",
    "            order.append(SB_DF.loc[SB_DF['1'] == child.attrib['sensor']]['Order'].values[0])\n",
    "            try:\n",
    "                stations.append(child.attrib['sensor'])\n",
    "            except KeyError:\n",
    "                stations.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                times.append(str(root.attrib['time_stamp']))\n",
    "            except KeyError:\n",
    "                times.append(\"NA\")\n",
    "            try:\n",
    "                flows.append(child.attrib['flow'])\n",
    "            except KeyError:\n",
    "                flows.append(\"NA\")\n",
    "\n",
    "            try:\n",
    "                speeds.append(child.attrib['speed'])\n",
    "            except KeyError:\n",
    "                speeds.append(\"NA\")\n",
    "            \n",
    "\n",
    "    DF = pd.DataFrame({\"Station\" : stations,\n",
    "                       \"Heading\": headings,\n",
    "                        \"Time\" : times,\n",
    "                       \"Order\" : order,\n",
    "                       \"Speed\" : speeds,\n",
    "                       \"Flow\" : flows})\n",
    "    with open(f'data/station_data.csv', 'a') as f:\n",
    "           DF.to_csv(f, header=True)\n",
    "    print(\"Station Data Parsed\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 10, 3, 10, 13, 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Adjust and parse time format\n",
    "##################################\n",
    "\n",
    "def time_xml2dt(time_xml):\n",
    "    from time import mktime\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    #time_xml='Wed Oct 03 10:13:27 CDT 2018'\n",
    "    B=time_xml.split()\n",
    "    B.pop(4)\n",
    "    B[4]=B[4][2:]\n",
    "    B_struct=time.strptime(' '.join(B), \"%a %b %d  %H:%M:%S %y\")\n",
    "    time_dt=datetime.fromtimestamp(mktime(B_struct))\n",
    "    return time_dt\n",
    "time_xml='Wed Oct 03 10:13:27 CDT 2018'\n",
    "time_xml2dt(time_xml)\n",
    "\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Route_Summary():\n",
    "    global All_Station_Data\n",
    "    try:\n",
    "        Summary = pd.read_csv('data/Route_Summary.csv')\n",
    "    except FileNotFoundError:\n",
    "        Summary = pd.DataFrame(columns=[\"Station\",\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\"])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    All_Station_Data = pd.read_csv('data/station_data.csv')\n",
    "    All_Station_Data = All_Station_Data.set_index('Station')\n",
    "    Summary = Summary.set_index('Station')   \n",
    "\n",
    "\n",
    "    for station in All_Station_Data.index:\n",
    "    #             Route_Summary.append(Station_Config.loc[station, ['Lat', 'Lng']])\n",
    "    # we can grab it here, but repeats. better to grab as needed when graphing\n",
    "            Summary_partial = All_Station_Data.loc[station, \n",
    "                                                       [\"Station\",\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\"]index]\n",
    "            Summary = Summary.append(Summary_partial,sort=True)\n",
    "\n",
    "#     Summary.sort_values(['Station','Time'])\n",
    "\n",
    "\n",
    "\n",
    "    with open('data/Route_Summary.csv', 'w') as f:\n",
    "        Summary.to_csv(f,header=True, columns=[\"Heading\", \"Time\",\"Order\",\"Speed\",\"Flow\"])\n",
    "                       \n",
    "    # for Summary in Route_Summary:\n",
    "        ## WHAT ARE WE DOING WITH THESE?##\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xanen\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reindex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-87d7cf826b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# stations()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRoute_Summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# All_Station_Data.head()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-209-1934afbb5977>\u001b[0m in \u001b[0;36mRoute_Summary\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/Route_Summary.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Heading\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Order\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Speed\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Flow\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# for Summary in Route_Summary:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reindex'"
     ]
    }
   ],
   "source": [
    "# stations()\n",
    "Route_Summary()\n",
    "# All_Station_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "#     decription = []\n",
    "    lats = []\n",
    "    lngs = []\n",
    "    station_list = []\n",
    "        \n",
    "    XMLfile = \"data/XMLs/station_config.xml\"\n",
    "    parsedXML = ET.parse(XMLfile)\n",
    "    root = parsedXML.getroot()\n",
    "      \n",
    "\n",
    "    for i in root.iter('corridor'):\n",
    "        for child in i:\n",
    "            try:\n",
    "                station_list.append(child.attrib['station_id'])\n",
    "\n",
    "            except KeyError:\n",
    "                station_list.append(\"no ID\")\n",
    "            try:\n",
    "                lats.append(child.attrib['lat'])\n",
    "            except KeyError:\n",
    "                 lats.append(\"no ID\")\n",
    "            try:\n",
    "                lngs.append(child.attrib['lon'])\n",
    "            except KeyError:\n",
    "                lngs.append(\"no ID\")\n",
    "\n",
    "#             try:\n",
    "#                 decription.append(child.attrib['description'])\n",
    "#             except KeyError:\n",
    "#                 decription.append(\"error\")\n",
    "\n",
    "\n",
    "\n",
    "    DF = pd.DataFrame({ \"Station\":station_list,\n",
    "    #                        \"Label\":decription,\n",
    "                       \"Lat\":lats, \"Lng\":lngs,})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open('data/station_config.csv', 'w') as f:\n",
    "        DF.to_csv(f, header=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Identify metro sensor configurations\n",
    "############################################\n",
    "# Request xml.gz file, decompress, decode\n",
    "# with the stat_config.xml, look for a matching station. If not found, write the new station ID to stat_config.csv\n",
    "try:\n",
    "        config()\n",
    "except FileNotFoundError:\n",
    "    c = requests.get('http://data.dot.state.mn.us/iris_xml/metro_config.xml.gz')\n",
    "    with open('data/XMLs/station_config.xml', 'w') as handle:\n",
    "        handle.write(gzip.decompress(c.content).decode('utf-8'))\n",
    "    Station_Config = pd.DataFrame(columns=['Station', 'Lat', 'Lng'])\n",
    "    with open('data/station_config.csv', 'w') as f:\n",
    "        Station_Config.to_csv(f, header=True)\n",
    "        config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Time</th>\n",
       "      <th>order</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Flow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S328</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>71</td>\n",
       "      <td>59</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1943</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>41</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1148</th>\n",
       "      <td>2.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>68</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S559</th>\n",
       "      <td>3.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S557</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>47</td>\n",
       "      <td>54</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S556</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>49</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S560</th>\n",
       "      <td>6.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>40</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S579</th>\n",
       "      <td>7.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>27</td>\n",
       "      <td>66</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S577</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>24</td>\n",
       "      <td>66</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S577</th>\n",
       "      <td>9.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S578</th>\n",
       "      <td>10.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>26</td>\n",
       "      <td>57</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S575</th>\n",
       "      <td>11.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S576</th>\n",
       "      <td>12.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>22</td>\n",
       "      <td>57</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S584</th>\n",
       "      <td>13.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S585</th>\n",
       "      <td>14.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>38</td>\n",
       "      <td>56</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S582</th>\n",
       "      <td>15.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S583</th>\n",
       "      <td>16.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S580</th>\n",
       "      <td>17.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>28</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S568</th>\n",
       "      <td>18.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S569</th>\n",
       "      <td>19.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>34</td>\n",
       "      <td>89</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S566</th>\n",
       "      <td>20.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>44</td>\n",
       "      <td>61</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S565</th>\n",
       "      <td>21.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>48</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S573</th>\n",
       "      <td>22.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S574</th>\n",
       "      <td>23.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>23</td>\n",
       "      <td>68</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S571</th>\n",
       "      <td>24.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>31</td>\n",
       "      <td>56</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S572</th>\n",
       "      <td>25.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>29</td>\n",
       "      <td>62</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S570</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>33</td>\n",
       "      <td>71</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S570</th>\n",
       "      <td>27.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>29</td>\n",
       "      <td>71</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S587</th>\n",
       "      <td>28.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>25</td>\n",
       "      <td>56</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Sun Oct 07 21:22:59 CDT 2018</td>\n",
       "      <td>43</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S10</th>\n",
       "      <td>55.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>54</td>\n",
       "      <td>76</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>56.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>75</td>\n",
       "      <td>61</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>57.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S14</th>\n",
       "      <td>58.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>57</td>\n",
       "      <td>65</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>59.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S19</th>\n",
       "      <td>60.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>65</td>\n",
       "      <td>59</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S18</th>\n",
       "      <td>61.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>63</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S20</th>\n",
       "      <td>62.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>66</td>\n",
       "      <td>81</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S21</th>\n",
       "      <td>63.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S23</th>\n",
       "      <td>64.0</td>\n",
       "      <td>SB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S43</th>\n",
       "      <td>65.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>84</td>\n",
       "      <td>65</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S46</th>\n",
       "      <td>66.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>78</td>\n",
       "      <td>63</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S45</th>\n",
       "      <td>67.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>79</td>\n",
       "      <td>60</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S48</th>\n",
       "      <td>68.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S47</th>\n",
       "      <td>69.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>77</td>\n",
       "      <td>64</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S49</th>\n",
       "      <td>70.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S51</th>\n",
       "      <td>71.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>70</td>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S50</th>\n",
       "      <td>72.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>72</td>\n",
       "      <td>62</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S53</th>\n",
       "      <td>73.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S52</th>\n",
       "      <td>74.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>69</td>\n",
       "      <td>62</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S54</th>\n",
       "      <td>75.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S57</th>\n",
       "      <td>76.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S56</th>\n",
       "      <td>77.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S59</th>\n",
       "      <td>78.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>58</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S58</th>\n",
       "      <td>79.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S60</th>\n",
       "      <td>80.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S62</th>\n",
       "      <td>81.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>53</td>\n",
       "      <td>48</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S61</th>\n",
       "      <td>82.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>54</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S64</th>\n",
       "      <td>83.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>52</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S63</th>\n",
       "      <td>84.0</td>\n",
       "      <td>NB</td>\n",
       "      <td>Tue Oct 09 22:03:59 CDT 2018</td>\n",
       "      <td>51</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0 Heading                          Time order    Speed  \\\n",
       "Station                                                                    \n",
       "S328            0.0      NB  Sun Oct 07 21:22:59 CDT 2018    71       59   \n",
       "S1943           1.0      SB  Sun Oct 07 21:22:59 CDT 2018    41  UNKNOWN   \n",
       "S1148           2.0      SB  Sun Oct 07 21:22:59 CDT 2018    68  UNKNOWN   \n",
       "S559            3.0      SB  Sun Oct 07 21:22:59 CDT 2018    39       52   \n",
       "S557            4.0      NB  Sun Oct 07 21:22:59 CDT 2018    47       54   \n",
       "S556            5.0      NB  Sun Oct 07 21:22:59 CDT 2018    49  UNKNOWN   \n",
       "S560            6.0      SB  Sun Oct 07 21:22:59 CDT 2018    40  UNKNOWN   \n",
       "S579            7.0      SB  Sun Oct 07 21:22:59 CDT 2018    27       66   \n",
       "S577            8.0      NB  Sun Oct 07 21:22:59 CDT 2018    24       66   \n",
       "S577            9.0      SB  Sun Oct 07 21:22:59 CDT 2018    25       66   \n",
       "S578           10.0      SB  Sun Oct 07 21:22:59 CDT 2018    26       57   \n",
       "S575           11.0      NB  Sun Oct 07 21:22:59 CDT 2018    22       60   \n",
       "S576           12.0      SB  Sun Oct 07 21:22:59 CDT 2018    22       57   \n",
       "S584           13.0      SB  Sun Oct 07 21:22:59 CDT 2018    33       62   \n",
       "S585           14.0      SB  Sun Oct 07 21:22:59 CDT 2018    38       56   \n",
       "S582           15.0      SB  Sun Oct 07 21:22:59 CDT 2018    30       64   \n",
       "S583           16.0      SB  Sun Oct 07 21:22:59 CDT 2018    31       59   \n",
       "S580           17.0      SB  Sun Oct 07 21:22:59 CDT 2018    28  UNKNOWN   \n",
       "S568           18.0      NB  Sun Oct 07 21:22:59 CDT 2018    35       37   \n",
       "S569           19.0      NB  Sun Oct 07 21:22:59 CDT 2018    34       89   \n",
       "S566           20.0      NB  Sun Oct 07 21:22:59 CDT 2018    44       61   \n",
       "S565           21.0      NB  Sun Oct 07 21:22:59 CDT 2018    48  UNKNOWN   \n",
       "S573           22.0      NB  Sun Oct 07 21:22:59 CDT 2018    26       82   \n",
       "S574           23.0      NB  Sun Oct 07 21:22:59 CDT 2018    23       68   \n",
       "S571           24.0      NB  Sun Oct 07 21:22:59 CDT 2018    31       56   \n",
       "S572           25.0      NB  Sun Oct 07 21:22:59 CDT 2018    29       62   \n",
       "S570           26.0      NB  Sun Oct 07 21:22:59 CDT 2018    33       71   \n",
       "S570           27.0      SB  Sun Oct 07 21:22:59 CDT 2018    29       71   \n",
       "S587           28.0      NB  Sun Oct 07 21:22:59 CDT 2018    25       56   \n",
       "S2             29.0      SB  Sun Oct 07 21:22:59 CDT 2018    43  UNKNOWN   \n",
       "...             ...     ...                           ...   ...      ...   \n",
       "S10            55.0      SB  Tue Oct 09 22:03:59 CDT 2018    54       76   \n",
       "S13            56.0      SB  Tue Oct 09 22:03:59 CDT 2018    75       61   \n",
       "S15            57.0      SB  Tue Oct 09 22:03:59 CDT 2018    61       65   \n",
       "S14            58.0      SB  Tue Oct 09 22:03:59 CDT 2018    57       65   \n",
       "S17            59.0      SB  Tue Oct 09 22:03:59 CDT 2018    62       45   \n",
       "S19            60.0      SB  Tue Oct 09 22:03:59 CDT 2018    65       59   \n",
       "S18            61.0      SB  Tue Oct 09 22:03:59 CDT 2018    63  UNKNOWN   \n",
       "S20            62.0      SB  Tue Oct 09 22:03:59 CDT 2018    66       81   \n",
       "S21            63.0      SB  Tue Oct 09 22:03:59 CDT 2018    67       66   \n",
       "S23            64.0      SB  Tue Oct 09 22:03:59 CDT 2018    71       71   \n",
       "S43            65.0      NB  Tue Oct 09 22:03:59 CDT 2018    84       65   \n",
       "S46            66.0      NB  Tue Oct 09 22:03:59 CDT 2018    78       63   \n",
       "S45            67.0      NB  Tue Oct 09 22:03:59 CDT 2018    79       60   \n",
       "S48            68.0      NB  Tue Oct 09 22:03:59 CDT 2018    75       75   \n",
       "S47            69.0      NB  Tue Oct 09 22:03:59 CDT 2018    77       64   \n",
       "S49            70.0      NB  Tue Oct 09 22:03:59 CDT 2018    73       62   \n",
       "S51            71.0      NB  Tue Oct 09 22:03:59 CDT 2018    70       51   \n",
       "S50            72.0      NB  Tue Oct 09 22:03:59 CDT 2018    72       62   \n",
       "S53            73.0      NB  Tue Oct 09 22:03:59 CDT 2018    67       69   \n",
       "S52            74.0      NB  Tue Oct 09 22:03:59 CDT 2018    69       62   \n",
       "S54            75.0      NB  Tue Oct 09 22:03:59 CDT 2018    65       70   \n",
       "S57            76.0      NB  Tue Oct 09 22:03:59 CDT 2018    61       62   \n",
       "S56            77.0      NB  Tue Oct 09 22:03:59 CDT 2018    63       64   \n",
       "S59            78.0      NB  Tue Oct 09 22:03:59 CDT 2018    58  UNKNOWN   \n",
       "S58            79.0      NB  Tue Oct 09 22:03:59 CDT 2018    60       70   \n",
       "S60            80.0      NB  Tue Oct 09 22:03:59 CDT 2018    55       56   \n",
       "S62            81.0      NB  Tue Oct 09 22:03:59 CDT 2018    53       48   \n",
       "S61            82.0      NB  Tue Oct 09 22:03:59 CDT 2018    54  UNKNOWN   \n",
       "S64            83.0      NB  Tue Oct 09 22:03:59 CDT 2018    52  UNKNOWN   \n",
       "S63            84.0      NB  Tue Oct 09 22:03:59 CDT 2018    51  UNKNOWN   \n",
       "\n",
       "            Flow  \n",
       "Station           \n",
       "S328         720  \n",
       "S1943    UNKNOWN  \n",
       "S1148    UNKNOWN  \n",
       "S559         680  \n",
       "S557         320  \n",
       "S556     UNKNOWN  \n",
       "S560     UNKNOWN  \n",
       "S579         320  \n",
       "S577         520  \n",
       "S577         520  \n",
       "S578         360  \n",
       "S575         520  \n",
       "S576         420  \n",
       "S584         520  \n",
       "S585         720  \n",
       "S582         920  \n",
       "S583         360  \n",
       "S580     UNKNOWN  \n",
       "S568         240  \n",
       "S569         360  \n",
       "S566         840  \n",
       "S565     UNKNOWN  \n",
       "S573         360  \n",
       "S574         480  \n",
       "S571         440  \n",
       "S572         440  \n",
       "S570         640  \n",
       "S570         640  \n",
       "S587         120  \n",
       "S2       UNKNOWN  \n",
       "...          ...  \n",
       "S10          120  \n",
       "S13          480  \n",
       "S15           60  \n",
       "S14          240  \n",
       "S17          180  \n",
       "S19          420  \n",
       "S18            0  \n",
       "S20          660  \n",
       "S21          360  \n",
       "S23          360  \n",
       "S43          360  \n",
       "S46          780  \n",
       "S45          540  \n",
       "S48          480  \n",
       "S47          360  \n",
       "S49           60  \n",
       "S51          400  \n",
       "S50          120  \n",
       "S53          270  \n",
       "S52          540  \n",
       "S54          330  \n",
       "S57         1020  \n",
       "S56          840  \n",
       "S59      UNKNOWN  \n",
       "S58          780  \n",
       "S60          840  \n",
       "S62         1800  \n",
       "S61      UNKNOWN  \n",
       "S64      UNKNOWN  \n",
       "S63      UNKNOWN  \n",
       "\n",
       "[257 rows x 6 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Station_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download complete\n",
      "Incident Data Parsed\n",
      "Station Data Parsed\n",
      "Parsing Complete, sleeping 30s\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 6 fields in line 3, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-2a769959d61e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdata_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parsing Complete, sleeping 30s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mRoute_Summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-184-e1107edfd04e>\u001b[0m in \u001b[0;36mRoute_Summary\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mAll_Station_Data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/station_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mAll_Station_Data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAll_Station_Data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Station'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mSummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Station'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 3, saw 7\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#If the program is still running, \n",
    "# Print the download is complete\n",
    "# Print the Parsing is Complete\n",
    "# Program sleep for 30 seconds\n",
    "# ####################################\n",
    "\n",
    "while True:\n",
    "    download()\n",
    "    print(\"download complete\")\n",
    "    data_check()\n",
    "    print(\"Parsing Complete, sleeping 30s\")\n",
    "    Route_Summary()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
